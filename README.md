This is a predictive analysis performed using python and the associated modules, on the census data from the UCI machine learning database, to predict the income category of individuals. 

Description of the data:
Source citation: The data represents the census data from the UCI machine learning database (http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data). Detailed information can be read by referring to the original publication (http://robotics.stanford.edu/~ronnyk/nbtree.pdf). There were 15 attributes, of which I have loaded 9 attributes/columns into a dataframe named 'adult' for further analyses. The 9 attributes were the age, workclass, fnlwgt, education-num, sex, capital-gain, capital-loss, hours-per-week and income columns.

Exploration and data cleaning: 
	Of the attributes, age, fnlwgt, capital-gain, capital-loss and hours-per-week were numerical attributes and the rest categorical. Initially there were 32561 rows. There were no null values in the numerical data columns, but were present in some categorical columns. Upon exploration it was seen that the education status of males and females followed a similar trend as the employment, being lower for women across job types. 
	I performed data cleaning by following the steps outlined below. The numerical column 'fnlwgt' had outliers, which were replaced with the median value, followed by z-standardization. A plotted histogram showed that this attribute has a more or less normal distribution. For the 'capital-gain' and 'capital-loss' columns, the rows with a few extreme values (>20000 for capital-gain and >3000 for capital loss) were removed. These two columns were also standardized by z-normalization. For the 'education-num', 'workclass' and 'sex' variables, the categories were decoded, imputed and consolidated. The 'education-num' variable was categorized into Primary (all education lesser than college), Bachelor, Master, Professional and Doctorate. The Doctoral was later consolidated into Professional. The 'workclass' column was categorized into Private, Government and Unemployed categories. The missing values in the 'workclass' column were imputed with the most frequent 'Private' value. The 'sex' column had two categories , 'Male' and 'Female'. Dummy variables were derived for the all three categorical variables and the parental columns deleted. The outliers in the age column were replaced with the median of the non-null values. For the 'age' and 'hours-per-week' columns, the values were binned into 1 (low), 2 (medium) and 3 (high) categories. The obsolete parent columns were deleted.

Explanantory analyses: 
	Following the data cleaning, there were 32297 rows and 15 columns in the 'adult' dataframe. Of the 15, 3 are z-standardized numerical columns named 'fnlwgt', 'capital-gain' and 'capital-loss'. 'Binned_age' and 'Binned_hours-per-week' are binned columns of the original numerical attributes. There were 2, 3 and 4 columns derived by one-hot encoding of the 'sex', 'workclass' and 'education-num' categorical columns respectively. The 'income' column represents the expert labels, claasifying each instance into income <= or > than 50000.
	The attributes used were as follows. fnlwgt: Numerical, z-standardized values capital-gain: Numerical, z-standardized values capital-loss: Numerical, z-standardized values Binned_age: The numerical 'age' column, binned and categorized Binned_hours-per-week: The numerical 'hours-per-week' column, binned and categorized Male: Categorical column denoting the gender Female: Categorical column denoting the gender Primary: Categorical column denoting the education level, derived from the 'education-num' attribute Bachelor: Categorical column denoting the education level, derived from the 'education-num' attribute Master: Categorical column denoting the education level, derived from the 'education-num' attribute Professional: Categorical column denoting the education level, derived from the 'education-num' attribute Private: One hot encoded category derived from the 'workclass' column Government: One hot encoded category derived from the 'workclass' column Unemployed: One hot encoded category derived from the 'workclass' column income: Expert label column denoting whether the calssification is <=50000 or >50000

Relevant questions:

    Do the age, education level or gender of the individuals affect their annual income?
    What is the effect of fnlwgt, capital-loss and capital-gain variables on the annual income?
    How much does work-hours of the individual influence the individual's annual income? For example, is there a direct correlation 	between the work-hours and income?
    Is the annual income affected by the workclass/employment status?

For performing an unsupervised K-means clustering, the 14 attributes in the adult dataframe except the 'income' column, were loaded into a new dataframe termed 'adult_k'. The new dataframe had 14 columns and all 32297 instances. After clustering, the K-means transformed 'labels' were appended as a 16th column to the original 'adult' dataframe.

I used the random forest and logistic regression classifiers to build and test the models. I found that excluding the fnlwgt column improved the accuracy of the predictions, so removed it. First, a Random Forest classification was performed to predict the income of the test data, after training the dataset with 80% of the data. I found that using the non-binned 'age' and 'hours-per-week' did not yield any worse result with the random forest classifier (data not shown) than with the binned variables. I used the default probability threshold of 0.5 for constructing the confusion matrix and the rest of the metrics. The preliminary analysis showed that the accuracy of the prediction was approximately 84%, using the random forest classifier on the attributes. The analysis metrics were printed along with the ROC curve. The AUC score was 0.70. With the logistic regression classifier, the accuracy of the prediction was approximately 83%. The AUC score was 0.71. Overall, I found that both the calssifiers showed more or less similar predictions, except that logistic regression showed a better recall of around 93%, while random forest maintained recall at 86%. The predicted labels and real values were then loaded into a new dataframe labeled 'adult_analysis', and saved as a csv file.

